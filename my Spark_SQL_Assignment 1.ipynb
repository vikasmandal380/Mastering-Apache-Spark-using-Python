{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3ee6f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct, sum, avg, when\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"SparkSQL_Assignment\").getOrCreate()\n",
    "\n",
    "train_df = spark.read.csv(\"hdfs://m01.itversity.com:9000/user/ana017398/train.csv\", header=True, inferSchema=True)\n",
    "fulfillment_df = spark.read.csv(\"hdfs://m01.itversity.com:9000/user/ana017398/fulfilment_center_info.csv\", header=True, inferSchema=True)\n",
    "meal_df = spark.read.csv(\"hdfs://m01.itversity.com:9000/user/ana017398/meal_info.csv\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# Register DataFrames as SQL tables\n",
    "train_df.createOrReplaceTempView(\"train\")\n",
    "fulfillment_df.createOrReplaceTempView(\"fulfillment\")\n",
    "meal_df.createOrReplaceTempView(\"meal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "832def9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "946fb6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|distinct_categories|distinct_cuisines|\n",
      "+-------------------+-----------------+\n",
      "|                 14|                4|\n",
      "+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT category) AS distinct_categories, \n",
    "       COUNT(DISTINCT cuisine) AS distinct_cuisines \n",
    "FROM meal\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2845d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f465266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|center_id|total_orders|\n",
      "+---------+------------+\n",
      "|       13|     1742220|\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT center_id, SUM(num_orders) AS total_orders\n",
    "FROM train\n",
    "GROUP BY center_id\n",
    "ORDER BY total_orders DESC\n",
    "LIMIT 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeb38474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "746d7bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|cuisine|total_orders|\n",
      "+-------+------------+\n",
      "|   Thai|      654724|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "WITH TopCenter AS (\n",
    "    SELECT center_id\n",
    "    FROM train\n",
    "    GROUP BY center_id\n",
    "    ORDER BY SUM(num_orders) DESC\n",
    "    LIMIT 1\n",
    ")\n",
    "SELECT m.cuisine, SUM(t.num_orders) AS total_orders\n",
    "FROM train t\n",
    "JOIN meal m ON t.meal_id = m.meal_id\n",
    "JOIN TopCenter tc ON t.center_id = tc.center_id\n",
    "GROUP BY m.cuisine\n",
    "ORDER BY total_orders DESC\n",
    "LIMIT 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27d29e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "337f2246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|center_type|       avg_op_area|\n",
      "+-----------+------------------+\n",
      "|     TYPE_C|3.1578947368421044|\n",
      "|     TYPE_B|4.7733333333333325|\n",
      "|     TYPE_A| 4.076744186046512|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT center_type, AVG(op_area) AS avg_op_area\n",
    "FROM fulfillment\n",
    "GROUP BY center_type\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a208f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9daccc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|center_type|      total_revenue|\n",
      "+-----------+-------------------+\n",
      "|     TYPE_A|7.276203201869873E9|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT f.center_type, SUM(t.checkout_price * t.num_orders) AS total_revenue\n",
    "FROM train t\n",
    "JOIN fulfillment f ON t.center_id = f.center_id\n",
    "GROUP BY f.center_type\n",
    "ORDER BY total_revenue DESC\n",
    "LIMIT 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb5d897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e043762e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|cuisine|total_orders|\n",
      "+-------+------------+\n",
      "|Italian|    17166334|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT m.cuisine, SUM(t.num_orders) AS total_orders\n",
    "FROM train t\n",
    "JOIN meal m ON t.meal_id = m.meal_id\n",
    "GROUP BY m.cuisine\n",
    "ORDER BY total_orders DESC\n",
    "LIMIT 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dab3394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc47f038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------+\n",
      "|week|    cuisine|total_orders|\n",
      "+----+-----------+------------+\n",
      "|   1|       Thai|      242088|\n",
      "|   1|    Italian|      228836|\n",
      "|   1|     Indian|      175317|\n",
      "|   1|Continental|      146020|\n",
      "|   2|       Thai|      273778|\n",
      "|   2|    Italian|      202627|\n",
      "|   2|     Indian|      177109|\n",
      "|   2|Continental|      133570|\n",
      "|   3|       Thai|      249838|\n",
      "|   3|    Italian|      197299|\n",
      "|   3|     Indian|      150148|\n",
      "|   3|Continental|       97977|\n",
      "|   4|       Thai|      277206|\n",
      "|   4|    Italian|      192265|\n",
      "|   4|     Indian|      155239|\n",
      "|   4|Continental|      118819|\n",
      "|   5|     Indian|      683532|\n",
      "|   5|       Thai|      229905|\n",
      "|   5|    Italian|      169161|\n",
      "|   5|Continental|      116077|\n",
      "+----+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT t.week, m.cuisine, SUM(t.num_orders) AS total_orders\n",
    "FROM train t\n",
    "JOIN meal m ON t.meal_id = m.meal_id\n",
    "GROUP BY t.week, m.cuisine\n",
    "ORDER BY t.week, total_orders DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02956cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76fc488d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|center_id|discount_count|\n",
      "+---------+--------------+\n",
      "|       13|          1509|\n",
      "+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT center_id, COUNT(*) AS discount_count\n",
    "FROM train\n",
    "WHERE checkout_price < base_price\n",
    "GROUP BY center_id\n",
    "ORDER BY discount_count DESC\n",
    "LIMIT 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e65179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
